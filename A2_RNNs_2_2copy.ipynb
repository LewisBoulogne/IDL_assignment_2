{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Deep Learning, Assignment 2, Task 2\n",
        "\n",
        "# Introduction\n",
        "\n",
        "\n",
        "The goal of this assignment is to learn how to use encoder-decoder recurrent neural networks (RNNs). Specifically we will be dealing with a sequence to sequence problem and try to build recurrent models that can learn the principles behind simple arithmetic operations (**integer addition, subtraction and multiplication.**).\n",
        "\n",
        "<img src=\"https://i.ibb.co/5Ky5pbk/Screenshot-2023-11-10-at-07-51-21.png\" alt=\"Screenshot-2023-11-10-at-07-51-21\" border=\"0\" width=\"500\"></a>\n",
        "\n",
        "In this assignment you will be working with three different kinds of models, based on input/output data modalities:\n",
        "1. **Text-to-text**: given a text query containing two integers and an operand between them (+ or -) the model's output should be a sequence of integers that match the actual arithmetic result of this operation\n",
        "2. **Image-to-text**: same as above, except the query is specified as a sequence of images containing individual digits and an operand.\n",
        "3. **Text-to-image**: the query is specified in text format as in the text-to-text model, however the model's output should be a sequence of images corresponding to the correct result.\n",
        "\n",
        "\n",
        "### Description\n",
        "Let us suppose that we want to develop a neural network that learns how to add or subtract\n",
        "two integers that are at most two digits long. For example, given input strings of 5 characters: ‘81+24’ or\n",
        "’41-89’ that consist of 2 two-digit long integers and an operand between them, the network should return a\n",
        "sequence of 3 characters: ‘105 ’ or ’-48 ’ that represent the result of their respective queries. Additionally,\n",
        "we want to build a model that generalizes well - if the network can extract the underlying principles behind\n",
        "the ’+’ and ’-’ operands and associated operations, it should not need too many training examples to generate\n",
        "valid answers to unseen queries. To represent such queries we need 13 unique characters: 10 for digits (0-9),\n",
        "2 for the ’+’ and ’-’ operands and one for whitespaces ’ ’ used as padding.\n",
        "The example above describes a text-to-text sequence mapping scenario. However, we can also use different\n",
        "modalities of data to represent our queries or answers. For that purpose, the MNIST handwritten digit\n",
        "dataset is going to be used again, however in a slightly different format. The functions below will be used to create our datasets.\n",
        "\n",
        "---\n",
        "\n",
        "*To work on this notebook you should create a copy of it.*\n",
        "\n",
        "When using the Lab Computers, download the Jupyter Notebook to one of the machines first.\n",
        "\n",
        "If you want to use Google Colab, you should first copy this notebook and enable GPU runtime in 'Runtime -> Change runtime type -> Hardware acceleration -> GPU **OR** TPU'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function definitions for creating the datasets\n",
        "\n",
        "First we need to create our datasets that are going to be used for training our models.\n",
        "\n",
        "In order to create image queries of simple arithmetic operations such as '15+13' or '42-10' we need to create images of '+' and '-' signs using ***open-cv*** library. We will use these operand signs together with the MNIST dataset to represent the digits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-05 17:49:13.057241: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-12-05 17:49:13.120383: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-12-05 17:49:18.492110: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.layers import Dense, RNN, LSTM, Flatten, TimeDistributed, LSTMCell\n",
        "from tensorflow.keras.layers import RepeatVector, Conv2D, SimpleRNN, GRU, Reshape, ConvLSTM2D, Conv2DTranspose, MaxPooling2D, Dropout, Activation, BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 200x200 with 25 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 200x200 with 25 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from scipy.ndimage import rotate\n",
        "\n",
        "\n",
        "# Create plus/minus operand signs\n",
        "def generate_images(number_of_images=50, sign='-'):\n",
        "    blank_images = np.zeros([number_of_images, 28, 28])  # Dimensionality matches the size of MNIST images (28x28)\n",
        "    x = np.random.randint(12, 16, (number_of_images, 2)) # Randomized x coordinates\n",
        "    y1 = np.random.randint(6, 10, number_of_images)       # Randomized y coordinates\n",
        "    y2 = np.random.randint(18, 22, number_of_images)     # -||-\n",
        "\n",
        "    for i in range(number_of_images): # Generate n different images\n",
        "        cv2.line(blank_images[i], (y1[i], x[i,0]), (y2[i], x[i, 1]), (255,0,0), 2, cv2.LINE_AA)     # Draw lines with randomized coordinates\n",
        "        if sign == '+':\n",
        "            cv2.line(blank_images[i], (x[i,0], y1[i]), (x[i, 1], y2[i]), (255,0,0), 2, cv2.LINE_AA) # Draw lines with randomized coordinates\n",
        "\n",
        "    return blank_images\n",
        "\n",
        "def show_generated(images, n=5):\n",
        "    plt.figure(figsize=(2, 2))\n",
        "    for i in range(n**2):\n",
        "        plt.subplot(n, n, i+1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(images[i])\n",
        "    plt.show()\n",
        "\n",
        "show_generated(generate_images())\n",
        "show_generated(generate_images(sign='+'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_data(highest_integer, num_addends=2, operands=['+', '-']):\n",
        "    \"\"\"\n",
        "    Creates the following data for all pairs of integers up to [1:highest integer][+/-][1:highest_integer]:\n",
        "\n",
        "    @return:\n",
        "    X_text: '51+21' -> text query of an arithmetic operation (5)\n",
        "    X_img : Stack of MNIST images corresponding to the query (5 x 28 x 28) -> sequence of 5 images of size 28x28\n",
        "    y_text: '72' -> answer of the arithmetic text query\n",
        "    y_img :  Stack of MNIST images corresponding to the answer (3 x 28 x 28)\n",
        "\n",
        "    Images for digits are picked randomly from the whole MNIST dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    num_indices = [np.where(MNIST_labels==x) for x in range(10)]\n",
        "    num_data = [MNIST_data[inds] for inds in num_indices]\n",
        "    image_mapping = dict(zip(unique_characters[:10], num_data))\n",
        "    image_mapping['-'] = generate_images()\n",
        "    image_mapping['+'] = generate_images(sign='+')\n",
        "    image_mapping['*'] = generate_images(sign='*')\n",
        "    image_mapping[' '] = np.zeros([1, 28, 28])\n",
        "\n",
        "    X_text, X_img, y_text, y_img = [], [], [], []\n",
        "\n",
        "    for i in range(highest_integer + 1):      # First addend\n",
        "        for j in range(highest_integer + 1):  # Second addend\n",
        "            for sign in operands: # Create all possible combinations of operands\n",
        "                query_string = to_padded_chars(str(i) + sign + str(j), max_len=max_query_length, pad_right=True)\n",
        "                query_image = []\n",
        "                for n, char in enumerate(query_string):\n",
        "                    image_set = image_mapping[char]\n",
        "                    index = np.random.randint(0, len(image_set), 1)\n",
        "                    query_image.append(image_set[index].squeeze())\n",
        "\n",
        "                result = eval(query_string)\n",
        "                result_string = to_padded_chars(result, max_len=max_answer_length, pad_right=True)\n",
        "                result_image = []\n",
        "                for n, char in enumerate(result_string):\n",
        "                    image_set = image_mapping[char]\n",
        "                    index = np.random.randint(0, len(image_set), 1)\n",
        "                    result_image.append(image_set[index].squeeze())\n",
        "\n",
        "                X_text.append(query_string)\n",
        "                X_img.append(np.stack(query_image))\n",
        "                y_text.append(result_string)\n",
        "                y_img.append(np.stack(result_image))\n",
        "\n",
        "    return np.stack(X_text), np.stack(X_img)/255., np.stack(y_text), np.stack(y_img)/255.\n",
        "\n",
        "def to_padded_chars(integer, max_len=3, pad_right=False):\n",
        "    \"\"\"\n",
        "    Returns a string of len()=max_len, containing the integer padded with ' ' on either right or left side\n",
        "    \"\"\"\n",
        "    length = len(str(integer))\n",
        "    padding = (max_len - length) * ' '\n",
        "    if pad_right:\n",
        "        return str(integer) + padding\n",
        "    else:\n",
        "        return padding + str(integer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Creating our data\n",
        "\n",
        "The dataset consists of 20000 samples that (additions and subtractions between all 2-digit integers) and they have two kinds of inputs and label modalities:\n",
        "\n",
        "  **X_text**: strings containing queries of length 5: ['  1+1  ', '11-18', ...]\n",
        "\n",
        "  **X_image**: a stack of images representing a single query, dimensions: [5, 28, 28]\n",
        "\n",
        "  **y_text**: strings containing answers of length 3: ['  2', '156']\n",
        "\n",
        "  **y_image**: a stack of images that represents the answer to a query, dimensions: [3, 28, 28]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Illustrate the generated query/answer pairs\n",
        "\n",
        "unique_characters = '0123456789+- '       # All unique characters that are used in the queries (13 in total: digits 0-9, 2 operands [+, -], and a space character ' '.)\n",
        "highest_integer = 99                      # Highest value of integers contained in the queries\n",
        "\n",
        "max_int_length = len(str(highest_integer))# Maximum number of characters in an integer\n",
        "max_query_length = max_int_length * 2 + 1 # Maximum length of the query string (consists of two integers and an operand [e.g. '22+10'])\n",
        "max_answer_length = 3    # Maximum length of the answer string (the longest resulting query string is ' 1-99'='-98')\n",
        "\n",
        "# Create the data (might take around a minute)\n",
        "(MNIST_data, MNIST_labels), _ = tf.keras.datasets.mnist.load_data()\n",
        "X_text, X_img, y_text, y_img = create_data(highest_integer)\n",
        "print(X_text.shape, X_img.shape, y_text.shape, y_img.shape)\n",
        "\n",
        "\n",
        "## Display the samples that were created\n",
        "def display_sample(n):\n",
        "    labels = ['X_img:', 'y_img:']\n",
        "    for i, data in enumerate([X_img, y_img]):\n",
        "        plt.subplot(1,2,i+1)\n",
        "        # plt.set_figheight(15)\n",
        "        plt.axis('off')\n",
        "        plt.title(labels[i])\n",
        "        plt.imshow(np.hstack(data[n]), cmap='gray')\n",
        "    print('='*50, f'\\nQuery #{n}\\n\\nX_text: \"{X_text[n]}\" = y_text: \"{y_text[n]}\"')\n",
        "    plt.show()\n",
        "\n",
        "for _ in range(10):\n",
        "    display_sample(np.random.randint(0, 10000, 1)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper functions\n",
        "\n",
        "The functions below will help with input/output of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot encoding/decoding the text queries/answers so that they can be processed using RNNs\n",
        "# You should use these functions to convert your strings and read out the output of your networks\n",
        "\n",
        "def encode_labels(labels, max_len=3):\n",
        "  n = len(labels)\n",
        "  length = len(labels[0])\n",
        "  char_map = dict(zip(unique_characters, range(len(unique_characters))))\n",
        "  one_hot = np.zeros([n, length, len(unique_characters)])\n",
        "  for i, label in enumerate(labels):\n",
        "      m = np.zeros([length, len(unique_characters)])\n",
        "      for j, char in enumerate(label):\n",
        "          m[j, char_map[char]] = 1\n",
        "      one_hot[i] = m\n",
        "\n",
        "  return one_hot\n",
        "\n",
        "\n",
        "def decode_labels(labels):\n",
        "    pred = np.argmax(labels, axis=2)\n",
        "    predicted = [''.join([unique_characters[i] for i in j]) for j in pred]\n",
        "\n",
        "    return predicted\n",
        "\n",
        "X_text_onehot = encode_labels(X_text)\n",
        "y_text_onehot = encode_labels(y_text)\n",
        "\n",
        "print(X_text_onehot.shape, y_text_onehot.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# I. Text-to-text RNN model\n",
        "\n",
        "The following code showcases how Recurrent Neural Networks (RNNs) are built using Keras. Several new layers are going to be used:\n",
        "\n",
        "1. LSTM\n",
        "2. TimeDistributed\n",
        "3. RepeatVector\n",
        "\n",
        "The code cell below explains each of these new components.\n",
        "\n",
        "<img src=\"https://i.ibb.co/NY7FFTc/Screenshot-2023-11-10-at-09-27-25.png\" alt=\"Screenshot-2023-11-10-at-09-27-25\" border=\"0\" width=\"500\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_text2text_model():\n",
        "\n",
        "    # We start by initializing a sequential model\n",
        "    text2text = tf.keras.Sequential()\n",
        "\n",
        "    # \"Encode\" the input sequence using an RNN, producing an output of size 256.\n",
        "    # In this case the size of our input vectors is [5, 13] as we have queries of length 5 and 13 unique characters. Each of these 5 elements in the query will be fed to the network one by one,\n",
        "    # as shown in the image above (except with 5 elements).\n",
        "    # Hint: In other applications, where your input sequences have a variable length (e.g. sentences), you would use input_shape=(None, unique_characters).\n",
        "    text2text.add(LSTM(256, input_shape=(None, len(unique_characters))))\n",
        "\n",
        "    # As the decoder RNN's input, repeatedly provide with the last output of RNN for each time step. Repeat 3 times as that's the maximum length of the output (e.g. '  1-99' = '-98')\n",
        "    # when using 2-digit integers in queries. In other words, the RNN will always produce 3 characters as its output.\n",
        "    text2text.add(RepeatVector(max_answer_length))\n",
        "\n",
        "    # By setting return_sequences to True, return not only the last output but all the outputs so far in the form of (num_samples, timesteps, output_dim). This is necessary as TimeDistributed in the below expects\n",
        "    # the first dimension to be the timesteps.\n",
        "    text2text.add(LSTM(256, return_sequences=True))\n",
        "\n",
        "    # Apply a dense layer to the every temporal slice of an input. For each of step of the output sequence, decide which character should be chosen.\n",
        "    text2text.add(TimeDistributed(Dense(len(unique_characters), activation='softmax')))\n",
        "\n",
        "    # Next we compile the model using categorical crossentropy as our loss function.\n",
        "    text2text.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    text2text.summary()\n",
        "\n",
        "    return text2text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Your code (look at the assignment description for your tasks for text-to-text model):\n",
        "## Your first task is to fit the text2text model using X_text and y_text\n",
        "text2text = build_text2text_model()\n",
        "\n",
        "X_train_tt, X_test_tt, Y_train_tt, Y_test_tt = train_test_split(X_text_onehot, y_text_onehot, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "history_tt = text2text.fit(\n",
        "    X_train_tt, Y_train_tt,\n",
        "    batch_size=32,\n",
        "    epochs=20,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "accuracy_tt = text2text.evaluate(X_test_tt, Y_test_tt)\n",
        "\n",
        "\n",
        "print(\"test loss, test acc:\", accuracy_tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history, title1, title2): \n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(title1)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(title2)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(12, 5))\n",
        "\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.plot(history_tt.history['accuracy'], label='Train Accuracy')\n",
        "# plt.plot(history_tt.history['val_accuracy'], label='Validation Accuracy')\n",
        "# plt.title('Accuracy for 50% train 50% test split')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(history_tt.history['loss'], label='Train Loss')\n",
        "# plt.plot(history_tt.history['val_loss'], label='Validation Loss')\n",
        "# plt.title('Loss for 50% train 50% test split')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "plot_history(history_tt, 'Accuracy for 50% train 50% test split', 'Loss for 50% train 50% test split')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text2text = build_text2text_model()\n",
        "\n",
        "X_train_tt, X_test_tt, Y_train_tt, Y_test_tt = train_test_split(X_text_onehot, y_text_onehot, test_size=0.75, random_state=42)\n",
        "\n",
        "\n",
        "history_tt = text2text.fit(\n",
        "    X_train_tt, Y_train_tt,\n",
        "    batch_size=32,\n",
        "    epochs=20,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "\n",
        "accuracy_tt = text2text.evaluate(X_test_tt, Y_test_tt)\n",
        "\n",
        "print(\"test loss, test acc:\", accuracy_tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(12, 5))\n",
        "\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.plot(history_tt.history['accuracy'], label='Train Accuracy')\n",
        "# plt.plot(history_tt.history['val_accuracy'], label='Validation Accuracy')\n",
        "# plt.title('Accuracy for 25% train 75% test split')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(history_tt.history['loss'], label='Train Loss')\n",
        "# plt.plot(history_tt.history['val_loss'], label='Validation Loss')\n",
        "# plt.title('Loss for 25% train 75% test split')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()\n",
        "plot_history(history_tt, 'Accuracy for 25% train 75% test split', 'Loss for 25% train 75% test split')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text2text = build_text2text_model()\n",
        "\n",
        "\n",
        "X_train_tt, X_test_tt, Y_train_tt, Y_test_tt = train_test_split(X_text_onehot, y_text_onehot, test_size=0.9, random_state=42)\n",
        "\n",
        "\n",
        "history_tt = text2text.fit(\n",
        "    X_train_tt, Y_train_tt,\n",
        "    batch_size=32,\n",
        "    epochs=20,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "\n",
        "accuracy_tt = text2text.evaluate(X_test_tt, Y_test_tt)\n",
        "\n",
        "print(\"test loss, test acc:\", accuracy_tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(12, 5))\n",
        "\n",
        "# plt.subplot(1, 2, 1)\n",
        "# plt.plot(history_tt.history['accuracy'], label='Train Accuracy')\n",
        "# plt.plot(history_tt.history['val_accuracy'], label='Validation Accuracy')\n",
        "# plt.title('Accuracy for 10% train 90% test split')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.subplot(1, 2, 2)\n",
        "# plt.plot(history_tt.history['loss'], label='Train Loss')\n",
        "# plt.plot(history_tt.history['val_loss'], label='Validation Loss')\n",
        "# plt.title('Loss for 10% train 90% test split')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "plot_history(history_tt, 'Accuracy for 10% train 90% test split', 'Loss for 10% train 90% test split')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "---\n",
        "\n",
        "# I. Image to text RNN Model\n",
        "\n",
        "Hint: There are two ways of building the encoder for such a model - again by using the regular LSTM cells (with flattened images as input vectors) or recurrect convolutional layers [ConvLSTM2D](https://keras.io/api/layers/recurrent_layers/conv_lstm2d/).\n",
        "\n",
        "The goal here is to use **X_img** as inputs and **y_text** as outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_img2text_model_1():\n",
        "    cnn = tf.keras.Sequential()\n",
        "\n",
        "    cnn.add(Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
        "    cnn.add(MaxPooling2D((2,2)))\n",
        "    cnn.add(Flatten())\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(TimeDistributed(cnn, input_shape=(5, 28, 28, 1)))\n",
        "    model.add(LSTM(256))\n",
        "    model.add(RepeatVector(max_answer_length))\n",
        "    model.add(LSTM(256, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(len(unique_characters), activation='softmax')))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_it, X_test_it, Y_train_it, Y_test_it = train_test_split(X_img, y_text_onehot, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img2text = build_img2text_model_1()\n",
        "img2text.summary()\n",
        "\n",
        "history_it = img2text.fit(\n",
        "    X_train_it, Y_train_it,\n",
        "    batch_size=32,\n",
        "    epochs=100,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "y_pred_it = img2text.evaluate(X_test_it, Y_test_it)\n",
        "\n",
        "print(\"test loss, test acc:\", y_pred_it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_it.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_it.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_it.history['loss'], label='Train Loss')\n",
        "plt.plot(history_it.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# III. Text to image RNN Model\n",
        "\n",
        "Hint: to make this model work really well you could use deconvolutional layers in your decoder (you might need to look up ***Conv2DTranspose*** layer). However, regular vector-based decoder will work as well.\n",
        "\n",
        "The goal here is to use **X_text** as inputs and **y_img** as outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# separating the text digits --> label of cnn\n",
        "label_map = {str(d): d for d in range(10)}\n",
        "label_map['-'] = 10\n",
        "label_map[' '] = 11\n",
        "\n",
        "def separate_labels(labels_set, label_map = label_map):\n",
        "    separated_labels = []\n",
        "    for s in labels_set:\n",
        "        for char in s:\n",
        "            separated_labels.append(label_map[char])\n",
        "    separated_labels = np.array(separated_labels)\n",
        "\n",
        "    return separated_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# helper funtion for plotting\n",
        "def display_digits(y_pred, y_true, query_set):\n",
        "    for i in range(len(y_pred)):\n",
        "        plt.figure(figsize=(8,4))\n",
        "        \n",
        "        for t in range(3):\n",
        "            # predicted\n",
        "            plt.subplot(2, 3, t+1)\n",
        "            plt.imshow(y_pred[i, t], cmap='gray')\n",
        "            plt.title(f\"Pred {t}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # true label\n",
        "            plt.subplot(2, 3, t+1 + 3)\n",
        "            plt.imshow(y_true[i, t], cmap='gray')\n",
        "            plt.title(f\"True {t}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.suptitle(f\"Query: '{query_set[i]}'\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train and test split\n",
        "indices = np.arange(len(X_text_onehot))\n",
        "\n",
        "train_idx, test_idx = train_test_split(\n",
        "    indices,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "X_train_ti = X_text_onehot[train_idx]\n",
        "X_test_ti  = X_text_onehot[test_idx]\n",
        "y_train_ti = y_img[train_idx]\n",
        "y_test_ti  = y_img[test_idx]\n",
        "y_test_labels_ti = y_text[test_idx]\n",
        "y_train_labels_ti = y_text[train_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model with regular vector-based decoder\n",
        "def build_text2img_model():\n",
        "    text2img = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=(None, len(unique_characters))),\n",
        "        LSTM(256),\n",
        "        RepeatVector(max_answer_length),\n",
        "        LSTM(256, return_sequences=True),\n",
        "        TimeDistributed(Dense(28*28, activation='sigmoid')),\n",
        "        TimeDistributed(Reshape((28, 28)))\n",
        "    ])\n",
        "\n",
        "    text2img.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    text2img.summary()\n",
        "\n",
        "    return text2img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text2img = build_text2img_model()\n",
        "history_t2i = text2img.fit(X_train_ti, \n",
        "                           y_train_ti,\n",
        "                           validation_split=0.2,\n",
        "                           epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict first 5 test images\n",
        "y_pred_img_reg = text2img.predict(X_test_ti[:5])\n",
        "y_true_reg = y_test_ti[:5]\n",
        "query_set_reg = X_text[test_idx[:5]]\n",
        "\n",
        "display_digits(y_pred_img_reg, y_true_reg, query_set_reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_text2img_model_conv(): \n",
        "    text2img_conv = tf.keras.Sequential([\n",
        "        # encoder\n",
        "        LSTM(512,input_shape=(None, len(unique_characters))),\n",
        "        # encoded vector \n",
        "        RepeatVector(max_answer_length),\n",
        "                     \n",
        "        # decoder \n",
        "        LSTM(512, return_sequences=True),\n",
        "        \n",
        "        BatchNormalization(), \n",
        "                                    \n",
        "        # transposed convolutions\n",
        "        # feature expansion \n",
        "        TimeDistributed(Dense(7*7*128, use_bias=True)), \n",
        "        TimeDistributed(BatchNormalization()),\n",
        "        Activation(tf.keras.activations.leaky_relu),\n",
        "\n",
        "        TimeDistributed(Reshape((7, 7, 128))),  \n",
        "        \n",
        "        # upsampling 1 -> 7x7 to 14x14\n",
        "        TimeDistributed(Conv2DTranspose(128, (3,3), strides=2, padding='same', use_bias=True)), \n",
        "        TimeDistributed(BatchNormalization()), \n",
        "        Activation(tf.keras.activations.leaky_relu),\n",
        "                                \n",
        "        # convolution layer   \n",
        "        TimeDistributed(Conv2D(64, (3,3), strides=1, padding='same')), \n",
        "        Activation(tf.keras.activations.leaky_relu),\n",
        "        \n",
        "        \n",
        "        # upsampling 2 -> 14x14 to 28x28\n",
        "        TimeDistributed(Conv2DTranspose(32, (3,3), strides=2, padding='same')), \n",
        "        TimeDistributed(BatchNormalization()), \n",
        "        Activation(tf.keras.activations.leaky_relu),\n",
        "         \n",
        "        # convolution layer -- output layer\n",
        "        TimeDistributed(Conv2D(1, (3,3), strides=1, padding='same', activation='sigmoid'))]) \n",
        "\n",
        "    text2img_conv.compile(loss='binary_crossentropy', optimizer='adam') \n",
        "    text2img_conv.summary() \n",
        "    return text2img_conv "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text2img_conv = build_text2img_model_conv() \n",
        "history_t2i_conv = text2img_conv.fit(X_train_ti, \n",
        "                                y_train_ti, \n",
        "                                validation_split = 0.2, \n",
        "                                epochs = 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_img_conv = text2img_conv.predict(X_test_ti[:5])\n",
        "y_true_conv = y_test_ti[:5]\n",
        "query_set_conv = X_text[test_idx[:5]]\n",
        "\n",
        "display_digits(y_pred_img_conv, y_true_conv, query_set_conv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cnn classifier for evaluating generated digits\n",
        "def build_digit_cnn_classifier():\n",
        "    cnn = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=[28, 28, 1]),\n",
        "        tf.keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\"),\n",
        "        tf.keras.layers.MaxPooling2D(2),\n",
        "        \n",
        "        tf.keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
        "        tf.keras.layers.MaxPooling2D(2),\n",
        "        \n",
        "        tf.keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
        "        tf.keras.layers.MaxPooling2D(2),\n",
        "        \n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        # \"0123456789- \"\n",
        "        tf.keras.layers.Dense(12, activation=\"softmax\")\n",
        "    ])\n",
        "    cnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                      optimizer=\"adam\",\n",
        "                      metrics=[\"accuracy\"])\n",
        "    \n",
        "    return cnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "separated_y_train_t2i = separate_labels(y_text)\n",
        "\n",
        "# separating the image digits --> features of cnn\n",
        "y_img_separated = y_img.reshape(-1,28,28,1)\n",
        "\n",
        "# separating the image digits --> to be used with prediction\n",
        "separated_y_test_t2i = separate_labels(y_test_labels_ti)\n",
        "\n",
        "X_train_CNN, X_test_CNN, y_train_CNN, y_test_CNN, = train_test_split(y_img_separated, separated_y_train_t2i, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# build and train cnn\n",
        "cnn_model = build_digit_cnn_classifier()\n",
        "\n",
        "history = cnn_model.fit(X_train_CNN, \n",
        "                        y_train_CNN, \n",
        "                        epochs=20,\n",
        "                        validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prediction == generation\n",
        "y_pred_img_reg = text2img.predict(X_test_ti)\n",
        "\n",
        "# separating predicted digits\n",
        "generated_images_reg = y_pred_img_reg.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# cnn prediction\n",
        "digit_preds_reg = cnn_model.predict(generated_images_reg)\n",
        "digit_preds_reg = np.argmax(digit_preds_reg, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prediction == generation\n",
        "y_pred_img_conv = text2img_conv.predict(X_test_ti)\n",
        "\n",
        "# separating predicted digits\n",
        "generated_images_conv = y_pred_img_conv.reshape(-1, 28, 28, 1)\n",
        "# cnn prediction\n",
        "digit_preds_conv = cnn_model.predict(generated_images_conv)\n",
        "digit_preds_conv = np.argmax(digit_preds_conv, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report_reg = classification_report(separated_y_test_t2i, digit_preds_reg)\n",
        "report_conv = classification_report(separated_y_test_t2i, digit_preds_conv)\n",
        "print(\"Classification Report for Dense Decoder:\")\n",
        "print(report_reg)\n",
        "print(\"Classification Report for ConvTranspose Decoder:\")\n",
        "print(report_conv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "# helper function - display confusion matrix heatmap\n",
        "def display_comparison_heapmaps(cm1, cm2, cm1_title, cm2_title):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    labels = [str(i) for i in range(10)] + ['-', ' ']\n",
        "\n",
        "    # --- Model 1: Dense decoder ---\n",
        "    sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
        "    axes[0].set_title(cm1_title)\n",
        "    axes[0].set_xlabel(\"Predicted\")\n",
        "    axes[0].set_ylabel(\"True\")\n",
        "\n",
        "    # --- Model 2: ConvTranspose decoder ---\n",
        "    sns.heatmap(cm2, annot=True, fmt='d', cmap='Purples',\n",
        "                xticklabels=labels, yticklabels=labels, ax=axes[1])\n",
        "    axes[1].set_title(cm2_title)\n",
        "    axes[1].set_xlabel(\"Predicted\")\n",
        "    axes[1].set_ylabel(\"True\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute confusion matrices\n",
        "cm_reg  = confusion_matrix(separated_y_test_t2i, digit_preds_reg)\n",
        "cm_conv = confusion_matrix(separated_y_test_t2i, digit_preds_conv)\n",
        "\n",
        "display_comparison_heapmaps(cm_reg,cm_conv,'Confusion Matrix — Dense Decoder (Vector-based)', 'Confusion Matrix — ConvTranspose Decoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "\n",
        "# IV. Additional LSTM layers\n",
        "\n",
        "Try adding additional LSTM layers to your encoder networks and see how the performance of your\n",
        " models changes. Try to explain these performance differences in the context of the mistakes that\n",
        " your network was making before. Tip: you should add a flag ”return sequences=True” to the first\n",
        " recurrent layer of your network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## I. Text to Text Model Extended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_text2text_model_extended():\n",
        "    text2text = tf.keras.Sequential()\n",
        "\n",
        "    text2text.add(LSTM(256, input_shape=(None, len(unique_characters)), return_sequences=True))\n",
        "    text2text.add(LSTM(256, return_sequences=True))\n",
        "    text2text.add(LSTM(256, return_sequences=True))\n",
        "    text2text.add(LSTM(256))\n",
        "\n",
        "    text2text.add(RepeatVector(max_answer_length))\n",
        "\n",
        "    text2text.add(LSTM(256, return_sequences=True))\n",
        "    text2text.add(LSTM(256, return_sequences=True))\n",
        "    text2text.add(LSTM(256, return_sequences=True))\n",
        "    text2text.add(LSTM(256, return_sequences=True))\n",
        "\n",
        "    text2text.add(TimeDistributed(Dense(len(unique_characters), activation='softmax')))\n",
        "\n",
        "    text2text.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    text2text.summary()\n",
        "\n",
        "    return text2text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text2text_extended = build_text2text_model_extended()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_tt, X_test_tt, Y_train_tt, Y_test_tt = train_test_split(X_text_onehot, y_text_onehot, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_t2t_extended = text2text_extended.fit(X_train_tt, \n",
        "                                Y_train_tt, \n",
        "                                validation_split = 0.2, \n",
        "                                epochs = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_tt_extended = decode_labels(text2text_extended.predict(X_test_tt))\n",
        "separated_pred_tt_extended = separate_labels(y_pred_tt_extended)\n",
        "\n",
        "y_pred_tt = decode_labels(text2text.predict(X_test_tt))\n",
        "separated_pred_tt = separate_labels(y_pred_tt)\n",
        "\n",
        "Y_test_tt = decode_labels(Y_test_tt)\n",
        "separated_test_tt = separate_labels(Y_test_tt)\n",
        "\n",
        "\n",
        "title_t2t = \"Baseline Text to Text Model\"\n",
        "title_t2t_extended =  \"Extended Text to Text Model\"\n",
        "\n",
        "report_t2t = classification_report(separated_test_tt, separated_pred_tt)\n",
        "print(\"Classification Report for\",title_t2t)\n",
        "print(report_t2t)\n",
        "report_t2t_extended = classification_report(separated_test_tt, separated_pred_tt_extended)\n",
        "print(\"Classification Report for\",title_t2t_extended)\n",
        "print(report_t2t_extended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm_t2t = confusion_matrix(separated_test_tt, separated_pred_tt)\n",
        "cm_t2t_extended = confusion_matrix(separated_test_tt, separated_pred_tt_extended)\n",
        "display_comparison_heapmaps(cm_t2t, cm_t2t_extended, title_t2t, title_t2t_extended)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## II. Image to Text Model Extended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_img2text_model_1_extended():\n",
        "    cnn = tf.keras.Sequential()\n",
        "\n",
        "    cnn.add(Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
        "    cnn.add(MaxPooling2D((2,2)))\n",
        "    cnn.add(Flatten())\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(TimeDistributed(cnn, input_shape=(5, 28, 28, 1)))\n",
        "\n",
        "    model.add(LSTM(256, return_sequences=True))\n",
        "    model.add(LSTM(256, return_sequences=True))\n",
        "    model.add(LSTM(256))\n",
        "    model.add(RepeatVector(max_answer_length))\n",
        "    model.add(LSTM(256, return_sequences=True))\n",
        "    model.add(LSTM(256, return_sequences=True))\n",
        "    model.add(LSTM(256, return_sequences=True))\n",
        "\n",
        "    model.add(TimeDistributed(Dense(len(unique_characters), activation='softmax')))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img2text_extended = build_img2text_model_1_extended()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_it, X_test_it, Y_train_it, Y_test_it = train_test_split(X_img, y_text_onehot, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_i2t_extended = img2text_extended.fit(\n",
        "    X_train_it, Y_train_it,\n",
        "    batch_size=32,\n",
        "    epochs=40,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_it_extended = decode_labels(img2text_extended.predict(X_test_it))\n",
        "separated_pred_it_extended = separate_labels(y_pred_it_extended)\n",
        "\n",
        "y_pred_it = decode_labels(img2text.predict(X_test_it))\n",
        "separated_pred_it = separate_labels(y_pred_it)\n",
        "\n",
        "Y_test_it = decode_labels(Y_test_it)\n",
        "separated_test_it = separate_labels(Y_test_it)\n",
        "\n",
        "\n",
        "title_i2t = \"Baseline Image to Text Model\"\n",
        "title_i2t_extended =  \"Extended Image to Text Model\"\n",
        "\n",
        "report_i2t = classification_report(separated_test_it, separated_pred_it)\n",
        "print(\"Classification Report for\",title_i2t)\n",
        "print(report_i2t)\n",
        "report_i2t_extended = classification_report(separated_test_it, separated_pred_it_extended)\n",
        "print(\"Classification Report for\",title_i2t_extended)\n",
        "print(report_i2t_extended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm_i2t = confusion_matrix(separated_test_it, separated_pred_it)\n",
        "cm_i2t_extended = confusion_matrix(separated_test_it, separated_pred_it_extended)\n",
        "display_comparison_heapmaps(cm_i2t, cm_i2t_extended, title_i2t, title_i2t_extended)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## III.Text to Image Model Extended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_text2img_model_conv_extended(): \n",
        "    text2img_conv = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=(5, len(unique_characters))),\n",
        "        # encoder\n",
        "        # in - (batch, 5, 13)\n",
        "        # out - (batch, 5, 512)\n",
        "        LSTM(512, return_sequences=True),\n",
        "        # in - (batch, 5, 512)\n",
        "        # out - (batch, 512)\n",
        "        # LSTM(512, return_sequences=True),\n",
        "        # LSTM(512, return_sequences=True),\n",
        "        LSTM(512),\n",
        "        \n",
        "        # encoded vector \n",
        "        RepeatVector(max_answer_length),\n",
        "                     \n",
        "        # decoder \n",
        "        # LSTM(512, return_sequences=True),\n",
        "        # LSTM(512, return_sequences=True),\n",
        "        LSTM(512, return_sequences=True),\n",
        "        LSTM(512, return_sequences=True),\n",
        "        \n",
        "        BatchNormalization(), \n",
        "                                        \n",
        "        # transposed convolutions\n",
        "        # feature expansion \n",
        "        TimeDistributed(Dense(7*7*128, use_bias=True)), \n",
        "        TimeDistributed(BatchNormalization()),\n",
        "        Activation(tf.keras.activations.leaky_relu),\n",
        "\n",
        "        TimeDistributed(Reshape((7, 7, 128))),  \n",
        "        \n",
        "        # upsampling 1 -> 7x7 to 14x14\n",
        "        TimeDistributed(Conv2DTranspose(128, (3,3), strides=2, padding='same', use_bias=True)), \n",
        "        TimeDistributed(BatchNormalization()), \n",
        "        Activation(tf.keras.activations.leaky_relu),\n",
        "                                \n",
        "        \n",
        "        # convolution layer   \n",
        "        TimeDistributed(Conv2D(64, (3,3), strides=1, padding='same')), \n",
        "        Activation(tf.keras.activations.leaky_relu),\n",
        "        \n",
        "        \n",
        "        # upsampling 2 -> 14x14 to 28x28\n",
        "        TimeDistributed(Conv2DTranspose(32, (3,3), strides=2, padding='same')), \n",
        "        TimeDistributed(BatchNormalization()), \n",
        "        Activation(tf.keras.activations.leaky_relu),\n",
        "        \n",
        "        \n",
        "        # convolution layer -- output layer\n",
        "        TimeDistributed(Conv2D(1, (3,3), strides=1, padding='same', activation='sigmoid'))]) \n",
        "\n",
        "    text2img_conv.compile(loss='binary_crossentropy', optimizer='adam') \n",
        "    text2img_conv.summary() \n",
        "    return text2img_conv "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text2img_conv_extended = build_text2img_model_conv_extended()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history_t2i_conv_extended = text2img_conv_extended.fit(X_train_ti, \n",
        "                                y_train_ti, \n",
        "                                validation_split = 0.2, \n",
        "                                epochs = 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_img_conv_extended = text2img_conv_extended.predict(X_test_ti[:5])\n",
        "y_true_conv_extended = y_test_ti[:5]\n",
        "query_set_conv_extended = X_text[test_idx[:5]]\n",
        "\n",
        "display_digits(y_pred_img_conv_extended, y_true_conv_extended, query_set_conv_extended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prediction == generation\n",
        "y_pred_img_conv_extended = text2img_conv_extended.predict(X_test_ti)\n",
        "\n",
        "# separating predicted digits\n",
        "generated_images_conv_extended = y_pred_img_conv_extended.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# cnn prediction\n",
        "digit_preds_conv_extended = cnn_model.predict(generated_images_conv_extended)\n",
        "digit_preds_conv_extended = np.argmax(digit_preds_conv_extended, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "report_conv_extended = classification_report(separated_y_test_t2i, digit_preds_conv_extended)\n",
        "print(\"Classification Report for Extended ConvTranspose Decoder:\")\n",
        "print(report_conv_extended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cm_conv_extended = confusion_matrix(separated_y_test_t2i, digit_preds_conv_extended)\n",
        "cm_conv = confusion_matrix(separated_y_test_t2i, digit_preds_conv)\n",
        "\n",
        "display_comparison_heapmaps(cm_conv, cm_conv_extended, \"Confusion Matrix ConvTranspose Decoder\", \"Confusion Matrix Extended ConvTranspose Decoder\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FZ7Xx__nJmj7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "IDL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
